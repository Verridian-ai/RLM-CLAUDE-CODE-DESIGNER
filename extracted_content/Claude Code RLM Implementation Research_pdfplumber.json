{
  "timestamp": "2026-01-05T05:13:39.781526",
  "extraction_metadata": {
    "page_count": 17,
    "metadata": {
      "Title": "Claude Code RLM Implementation Research",
      "Producer": "Skia/PDF m145 Google Docs Renderer"
    },
    "method": "pdfplumber"
  },
  "content": "\n--- PAGE 1 ---\nImplementing the 'Cannot Fail' Recursive\nLanguage Model (RLM) Architecture\nwithin Claude Code: A Feasibility Study\nand Implementation Framework\nExecutive Summary\nThe trajectory of Large Language Model (LLM) development has recently encountered a\npersistent and non-trivial bottleneck: the effective context window. While frontier models now\nadvertise input capacities exceeding one million tokens, rigorous empirical analysis\ndemonstrates a phenomenon known as \"context rot,\" where reasoning capabilities degrade\nsignificantly as information density increases and the position of critical information shifts\ndeeper into the prompt context.1 This degradation poses a fundamental challenge for\nlong-horizon tasks such as deep research, codebase refactoring, and complex legal analysis,\nwhere the integrity of information retrieval must remain absolute regardless of the input\nvolume. The Recursive Language Model (RLM) framework, as proposed by Zhang et al., offers\na paradigm shift to address this limitation. By treating the prompt not as an immediate, holistic\ninput to the transformer but as an external environment variable to be programmatically\nqueried, the RLM separates the storage of information from the processing of reasoning.1\nThis report investigates the feasibility of transplanting the theoretical RLM architecture into\nClaude Code, Anthropic's agentic command-line interface. Our analysis suggests that\nClaude Code provides a uniquely suitable substrate for a \"Cannot Fail\" RLM implementation\ndue to its persistent shell environment, robust lifecycle hook system, and native sub-agent\ncapabilities.2 Unlike standard chat interfaces or Integrated Development Environment (IDE)\nplugins, Claude Code’s Read-Eval-Print Loop (REPL) nature aligns perfectly with the RLM\nrequirement for a persistent execution environment capable of symbolic manipulation and\nrecursive self-invocation.1\nWe propose a robust implementation architecture that leverages Claude Code’s SessionStart\nhooks to initialize an \"RLM Kernel,\" utilizes local file systems for \"out-of-core\" memory\nmanagement, and employs sub-agents for recursive depth handling. This \"Cannot Fail\"\nsystem is designed to prevent context overflow errors and reasoning hallucinations by\nenforcing a strict regime of programmatic context decomposition and verification. The\nfollowing analysis details the theoretical underpinnings, the specific affordances of the\nClaude Code platform, and a comprehensive implementation strategy that bridges the gap\nbetween academic theory and production-grade software engineering.\n--- PAGE 2 ---\n1. Theoretical Foundation: The Recursive Language\nModel\nTo validate the feasibility of the RLM architecture within the specific constraints and\ncapabilities of Claude Code, one must first dissect the theoretical requirements established\nby the foundational research. The RLM is not merely an agentic workflow; it is a fundamental\ninference strategy designed to scale inference-time compute to handle inputs up to two\norders of magnitude beyond the model's native context window.1\n1.1 The Environmental Prompt Hypothesis\nThe central insight driving the RLM architecture is that massive prompts should never be fed\ndirectly into the neural network's transformer layers in their entirety. Instead, the prompt $P$\nis treated as an immutable variable residing in an external programming environment.1 The\nLLM acts as a controller that interacts with $P$ through a Read-Eval-Print Loop (REPL),\nmirroring the way a human analyst might query a database rather than memorizing it.\nThis interaction is characterized by symbolic interaction, where the model writes code to\ninspect the properties of $P$ (e.g., determining length via len(P) or previewing content with\nP[:1000]) rather than reading the text linearly. This allows the model to form a schema of the\ninformation before committing computational resources to processing it. Furthermore, the\nmodel employs programmatic decomposition, utilizing string manipulation or regular\nexpressions to split $P$ into manageable chunks ($c_1, c_2,..., c_n$). This decomposition is\nnot arbitrary; it is guided by the model's initial symbolic inspection, ensuring that split points\nrespect the semantic structure of the data.1 Finally, the architecture relies on recursive\ninvocation, where the model calls a function—denoted as llm_query(query, chunk) in the\nliterature—which spawns a new independent RLM instance to process the specific chunk. This\neffectively isolates the reasoning context, ensuring that no single instance is overwhelmed by\nirrelevant data.1\nThis structure effectively transforms an $O(N^2)$ attention problem (where processing cost\nscales quadratically with input length due to the attention mechanism) into an out-of-core\ndata processing task. The processing cost shifts to a linear or log-linear function of the\nnumber of recursive calls required to distill the information, decoupling the size of the dataset\nfrom the complexity of the reasoning task.1\n1.2 Failure Modes in Standard Architectures\nThe \"Cannot Fail\" designation in our proposed architecture is derived from its ability to\nstructurally address the primary failure modes identified in standard Long-Context LLMs.\nContext Rot is the primary adversary. Empirical studies show that even in models with\n\"infinite\" context windows, performance degrades as the location of the \"needle\" (relevant\n--- PAGE 3 ---\ninformation) shifts or as the volume of the \"haystack\" (irrelevant context) increases.1 This is\nnot merely a retrieval failure but a reasoning failure; the noise of the haystack interferes with\nthe model's ability to attend to the signal. By compartmentalizing data into discrete chunks,\nthe RLM ensures that the signal-to-noise ratio in any given inference pass remains high.\nAttention Dilution occurs in tasks requiring global aggregation, such as \"Find all pairs of\nusers who interacted with...\" In standard models, maintaining focus across millions of tokens\nto track distributed entities is computationally prohibitive and prone to error. The RLM handles\nthis by aggregating intermediate results programmatically, shifting the burden of \"memory\"\nfrom the model's activations to a structured data store (e.g., a list or database) managed by\nthe environment.1\nMemory Overflow represents the hard physical limit of current hardware. Loading 10 million\ntokens into a single inference pass is currently impossible on standard deployment hardware\ndue to KV-cache limitations. The RLM architecture mitigates these by strictly enforcing that\nno single model instance ever sees more context than it can reliably reason over (e.g., <100k\ntokens), delegating the rest to sub-calls.1 The system \"cannot fail\" due to context limits\nbecause it never attempts to ingest the full context at once.\n2. Claude Code Environment Feasibility Analysis\nClaude Code, Anthropic's developer-focused Command Line Interface (CLI) tool, presents a\nunique operational environment that mirrors the theoretical requirements of an RLM more\nclosely than standard chat interfaces, IDE plugins, or even traditional agent frameworks. Its\narchitecture as a persistent, stateful shell session is key to this alignment.\n2.1 The Persistent REPL Environment\nThe RLM requires a persistent environment where variables (specifically the prompt variable)\ncan reside in memory or on disk while the model iterates through its reasoning loops. Claude\nCode operates as a stateful CLI session that maintains a persistent bash shell and directory\ncontext.2 This persistence is not merely a convenience; it is an architectural necessity for RLM.\nThe Bash Tool is Claude Code’s primary interface with the host system. It allows the agent to\nexecute system commands, read files, and pipe outputs.6 This functionality is equivalent to\nthe Python REPL described in the RLM paper but offers arguably greater power by accessing\nthe full Operating System (OS) toolset. Tools like grep, ripgrep, and awk allow for high-speed\n\"filtering\" of context before it is ever read into the model's context window, acting as a\npre-attention mechanism managed by the OS kernel rather than the transformer.\nFurthermore, the introduction of the Code Execution Tool (currently in beta) enables\nsandboxed Python execution.7 This allows for the precise import capabilities and variable\nmanipulation required by the RLM \"Kernel.\" Crucially, this Python environment can be\nconfigured to maintain state across turns, or at minimum, state can be serialized to disk and\n--- PAGE 4 ---\nreloaded, satisfying the requirement for variable persistence.9\n2.2 Recursion via Sub-agents and CLI\nRecursion is the engine of the RLM, enabling the system to \"drill down\" into data. Claude Code\nsupports this through two distinct and complementary mechanisms.\nFirst, Native Sub-agents provide a structured way to delegate tasks. Claude Code can spawn\nspecialized \"Sub-agents\" (via the Task tool) to handle specific sub-tasks with isolated context\nwindows.3 This directly maps to the llm_query recursive call described in the RLM literature.\nEach sub-agent operates independently, processing its assigned chunk of data without\npolluting the context of the parent agent. The parent agent acts as the orchestrator, receiving\nonly the distilled output.10\nSecond, Recursive CLI Calls offer a more raw, process-level form of recursion. Since Claude\nCode is a CLI tool, it can theoretically invoke itself (claude -p \"query\") from within its own\nbash shell.11 This creates a process-level recursion stack, where each child process is a\ncompletely fresh instance of the Claude Code agent. This method offers extreme isolation but\ncomes with higher overhead. It is particularly useful for \"headless\" operations where a\nsub-task needs to be executed purely for its side effects (e.g., generating a file) or its\nstandard output.13\n2.3 Context Management and \"Out-of-Core\" Storage\nFor the system to be \"Cannot Fail,\" it must handle prompts larger than the context window\nwithout crashing or truncating critical data. Claude Code excels in this regard by treating the\nFilesystem as Memory. Unlike a browser chat, which exists in a sandbox isolated from local\nstorage, Claude Code has direct read/write access to the local filesystem. A 100MB text file\n(approx. 25 million tokens) can sit on the disk—effectively \"Out-of-Core\"—and be queried via\ngrep or read commands without ever being fully loaded into the conversation history.14\nWhile Claude Code has built-in mechanisms for Context Compaction (/compact), which\nsummarizes history to save tokens 16, the RLM architecture circumvents the need for\naggressive, lossy compaction by keeping the primary data external. The context window is\nreserved for reasoning steps and code, not for raw data storage. This aligns with the\n\"Just-in-Time\" (JIT) retrieval strategy discussed in recent analyses of agentic workflows,\nwhere information is fetched only at the exact moment of necessity.14\n2.4 Comparative Capability Analysis\nTo rigorously evaluate Claude Code's suitability, we must compare its features against the\nstrict requirements of the RLM specification and contrast them with standard LLM inference\nmethods.\n--- PAGE 5 ---\nThe following table details the mapping between RLM requirements and Claude Code's\ncapabilities, highlighting the feasibility of the proposed architecture.\nRLM Requirement Specification Claude Code Feasibility\nDetail Capability Assessment\nPersistent REPL Environment must High: Persistent Confirmed: Native\nmaintain state Bash shell 6; Python Bash persistence is\n(variables) between REPL via MCP or robust; Python\ninference steps. Code Execution.9 persistence\nState can be achievable via MCP\nserialized to disk. or serialization.\nRecursive Model must be able High: Native Confirmed: Dual\nInvocation to spawn Sub-agents (Task mechanisms exist.\nindependent tool) 3; Recursive Sub-agents are\ninstances of itself. CLI calls (claude preferred for\n-p).12 context isolation;\nCLI for process\nisolation.\nExternal Context Prompt must be High: Direct Confirmed:\ntreated as an filesystem access. Filesystem acts as\nexternal variable, Data resides on infinite L2 cache.\nnot input tokens. disk grep/read serve as\n(\"Out-of-Core\") the interface.\nand is queried via\ntools.14\nSymbolic Model must be able High: Full access to Confirmed: The\nManipulation to inspect and split standard Linux/Unix environment\nthe prompt tools (awk, sed, provides superior\nprogrammatically. split, python).6 manipulation tools\ncompared to a\nstandard Python\nsandbox.\nDeterministic System must Medium/High: Confirmed: Hooks\nGuardrails prevent Hook system provide the\nunauthorized direct (PreToolUse, necessary\n[TABLES ON PAGE 5]\n\nTable 1:\nRLM Requirement | Specification\nDetail | Claude Code\nCapability | Feasibility\nAssessment\nPersistent REPL | Environment must\nmaintain state\n(variables) between\ninference steps. | High: Persistent\nBash shell 6; Python\nREPL via MCP or\nCode Execution.9\nState can be\nserialized to disk. | Confirmed: Native\nBash persistence is\nrobust; Python\npersistence\nachievable via MCP\nor serialization.\nRecursive\nInvocation | Model must be able\nto spawn\nindependent\ninstances of itself. | High: Native\nSub-agents (Task\ntool) 3; Recursive\nCLI calls (claude\n-p).12 | Confirmed: Dual\nmechanisms exist.\nSub-agents are\npreferred for\ncontext isolation;\nCLI for process\nisolation.\nExternal Context | Prompt must be\ntreated as an\nexternal variable,\nnot input tokens. | High: Direct\nfilesystem access.\nData resides on\ndisk\n(\"Out-of-Core\")\nand is queried via\ntools.14 | Confirmed:\nFilesystem acts as\ninfinite L2 cache.\ngrep/read serve as\nthe interface.\nSymbolic\nManipulation | Model must be able\nto inspect and split\nthe prompt\nprogrammatically. | High: Full access to\nstandard Linux/Unix\ntools (awk, sed,\nsplit, python).6 | Confirmed: The\nenvironment\nprovides superior\nmanipulation tools\ncompared to a\nstandard Python\nsandbox.\nDeterministic\nGuardrails | System must\nprevent\nunauthorized direct | Medium/High:\nHook system\n(PreToolUse, | Confirmed: Hooks\nprovide the\nnecessary\n\n--- PAGE 6 ---\nreading of massive SessionStart) can interception layer\nprompts. intercept and block to enforce the\ndangerous \"Cannot Fail\"\ncommands.18 protocols.\nThis evaluation confirms that Claude Code meets all critical infrastructure requirements for an\nRLM. The specific combination of persistent shell access, filesystem control, and lifecycle\nhooks creates an environment where the abstract components of the RLM can be instantiated\nas concrete software artifacts.\n3. The 'Cannot Fail' RLM Embedded Architecture\nThe \"Cannot Fail\" architecture defines a system where the risk of context overflow or\nreasoning failure is structurally eliminated by the environment setup. We define this\narchitecture as The RLM-C (Recursive Language Model on Claude) System. This system\nrelies on an \"Inversion of Control\" where the user does not prompt the model directly with\n[TABLES ON PAGE 6]\n\nTable 1:\n | reading of massive\nprompts. | SessionStart) can\nintercept and block\ndangerous\ncommands.18 | interception layer\nto enforce the\n\"Cannot Fail\"\nprotocols.\n\n--- PAGE 7 ---\ndata, but rather initializes a kernel that mediates all data interaction.\n3.1 The RLM Kernel: Inversion of Control\nIn a standard session, the user pastes a prompt into Claude. In the RLM-C architecture, we\ninvert this. The session is initialized with a \"Kernel\"—a Python or Bash script that mediates all\ninteraction between the model and the data.1\nThe Kernel is loaded immediately upon session start via the SessionStart hook.18 This hook is\nthe linchpin of the architecture; it injects the RLM primitives (functions for chunking, querying,\nand aggregating) into the active environment before the user can even issue a command. This\nensures that the RLM capabilities are omnipresent and that the environment is pre-configured\nto handle large data.\nThe Prompt Variable is redefined. The massive input context is not pasted into the chat.\nInstead, it is saved to a persistent file (e.g., context.txt, corpus.sqlite, or a specialized .mcp\nstorage) in the workspace. The Kernel exposes a handle to this file (e.g., an environment\nvariable CTX_FILE=\"context.txt\"). The model interacts with this handle, not the content itself.\nCrucially, the Root Instruction is embedded in the system prompt (via CLAUDE.md or the\n--system-prompt flag). This instruction explicitly forbids Claude from reading CTX_FILE\ndirectly. Instead, it mandates that Claude interacts with the data only via the Kernel tools.1 This\ninstruction acts as the \"prime directive\" of the RLM-C system, enforcing the architectural\nconstraints at the inference level.\n3.2 Recursive Depth via Sub-Agents\nThe RLM paper utilizes llm_query for recursion. In Claude Code, this maps directly to the\nSub-agent architecture.3\nThe Supervisor (Root Agent) holds the high-level goal (e.g., \"Find the answer in this 10MB\nbook\"). It does not attempt to read the book. Instead, it executes Kernel code to split the book\ninto chapters or logical sections. It acts as a manager, distributing work rather than doing it.\nThe Worker (Sub-agent) is instantiated by the Supervisor for each chunk of work. The\nSupervisor calls a sub-agent (using natural language delegation or the Agent tool) for each\nchapter.\n● Input: The sub-agent receives only the specific chunk (Chapter 1) and the specific query\nrelevant to that chunk.\n● Constraint: The sub-agent starts with a fresh, empty context window. It processes only\nthe chunk it is given, ensuring that its reasoning capacity is not diluted by the rest of the\ndataset.\n● Output: The sub-agent returns a concise \"Sub-Response\" to the Supervisor. This\nresponse is a distilled synthesis of the chunk, not a reproduction of it.\n--- PAGE 8 ---\nTo ensures \"Cannot Fail\" reliability, the Kernel enforces a Max Recursion Depth and a Token\nBudget for each sub-agent call.10 If a sub-agent attempts to spawn further sub-agents\nbeyond a safe depth (e.g., 3 levels), the Kernel blocks the request, preventing infinite loops or\n\"fork bomb\" scenarios where costs explode exponentially.\n3.3 Out-of-Core Memory Management\nTo handle 10M+ tokens, the system treats the local filesystem as a Level 2 (L2) cache, while\nthe context window acts as a Level 1 (L1) cache.\nIndexing is the first step of ingestion. Upon receiving a dataset, the Kernel uses tools like\nripgrep or a lightweight vector store (via MCP or local Python libraries like chromadb or\nsqlite-vec) to create an index of the data.22 This index allows for rapid, targeted retrieval\nwithout linear scanning.\nPaging is used when a sub-agent needs data. The Kernel \"pages\" data in from the L2 cache\n(filesystem). It reads specific lines, bytes, or logical blocks from the file, ensuring the result\nfits comfortably within the sub-agent's 200k token context limit.\nAggregation of results is handled carefully. Results from sub-agents are not simply\nconcatenated into the Supervisor's context window, as this would eventually cause an\noverflow. Instead, they are written to a results.json file on disk. The Supervisor then reads\nsummaries of these results or queries the results file iteratively to build the final answer.1 This\nensures that the Supervisor's context remains stable regardless of the amount of work\nperformed by the sub-agents.\n4. Implementation Strategy: A robust \"How-To\"\nThis section details the practical implementation of the RLM-C architecture using Claude\nCode’s configuration capabilities. It translates the architectural concepts into specific code\nand configuration steps.\n4.1 Step 1: The Environment Configuration (Hooks)\nThe foundation of the system is the SessionStart hook. This hook ensures the RLM\nenvironment is active in every session, guaranteeing the \"Cannot Fail\" state by preventing the\nuser (or Claude) from accidentally entering a standard, non-recursive mode where they might\nattempt to read a massive file directly.\nWe configure the .claude/settings.json file to load our environment script. This JSON\nconfiguration defines the triggers for our RLM initialization.\n--- PAGE 9 ---\nJSON\n{\n\"hooks\": {\n\"SessionStart\": [\n{\n\"matcher\": \"startup\",\n\"hooks\": [\n{\n\"type\": \"command\",\n\"command\": \"source.claude/rlm_kernel.sh && python3.claude/init_rlm.py\"\n}\n]\n}\n],\n\"PreToolUse\":\n}\n]\n}\n}\nThe SessionStart hook initializes the environment.18 It runs a shell script to set environment\nvariables (like CTX_FILE) and a Python script (init_rlm.py) that might start a background MCP\nserver or define helper functions. It is crucial to note that SessionStart hooks can be brittle;\ntheir stdout is not always visible to the user, and they must rely on side effects (like setting\nenvironment variables or creating files) to be effective.19\nThe PreToolUse hook acts as the system's safety valve or firewall.18 It intercepts every\nattempt by Claude to use the Read tool. The guardrails.py script checks the size of the target\nfile. If the file size exceeds a safe threshold (e.g., 500KB), the hook blocks the read operation\nand returns a system message to Claude: \"File too large for direct context. Use rlm_query or\nchunk_read instead.\" This feedback loop forces the model to adopt the RLM pattern,\neffectively \"teaching\" it the constraints of the environment.\n4.2 Step 2: The RLM Kernel (Python/MCP)\nThe Kernel is the set of tools exposed to Claude. While raw Bash scripts can suffice for simple\ntasks, a Python-based Model Context Protocol (MCP) server or a set of Python scripts is more\nrobust for complex data manipulation. For a lightweight embedded system, we define these\nas simple Python scripts that Claude executes via the Bash tool or the Code Execution\nenvironment.\n[TABLES ON PAGE 9]\n\nTable 1:\n{\n\"hooks\": {\n\"SessionStart\": [\n{\n\"matcher\": \"startup\",\n\"hooks\": [\n{\n\"type\": \"command\",\n\"command\": \"source.claude/rlm_kernel.sh && python3.claude/init_rlm.py\"\n}\n]\n}\n],\n\"PreToolUse\":\n}\n\nTable 2:\n}\n}\n\n--- PAGE 10 ---\nKey Kernel functions implemented in rlm_lib.py include:\n1. chunk_data(source_path, chunk_size=5000): This function splits a large source file\ninto temporary chunk files (e.g., .cache/chunk_001.txt). It manages the \"physical\"\ndecomposition of the prompt variable.\n2. delegate_task(instruction, context_file): This is the critical recursive function.\n○ It constructs a tailored prompt: \"You are a Sub-agent processing a chunk. Read\n{context_file}. Answer: {instruction}.\"\n○ It then calls claude -p (Print Mode) or invokes a Sub-agent via the Agent tool to\nprocess this single chunk.12\n○ Crucially, it captures the text output of this sub-call and writes it to a partial_result\nvariable or file, preventing it from flooding the current context.\n3. aggregate_results(pattern): This function reads all partial results generated by the\nsub-agents and prepares them for final synthesis, typically by concatenating them into a\nnew summary file or database view.\n4.3 Step 3: Configuring the System Prompt (CLAUDE.md)\nThe CLAUDE.md file serves as the \"BIOS\" or operating manual for the RLM system. It instructs\nthe model on its own architecture, defining the rules of engagement.26 Without this, the model\nhas no way of knowing it is operating within an RLM constraint system.\nProposed CLAUDE.md Content:\n\"You are running in RLM Mode. You DO NOT have infinite context. You have access\nto a massive external 'Environment'.\nPROTOCOL:\n1. Never read large files directly. Check file size with ls -lh first.\n2. Use the Kernel. To process large data, use python3 rlm_lib.py chunk...\n3. Recursive Delegation. If a task covers >5 files or >50k tokens, you MUST\nspawn a sub-task for each component using delegate_task.\n4. Verification. Before answering, verify your findings by running a specific\ngrep or python check.\nFailure to follow this protocol will trigger the PreToolUse guardrails.\"\nThis prompt explicitly aligns the model's behavior with the constraints enforced by the hooks,\ncreating a coherent system where the model's \"internal\" instructions match the \"external\"\nreality of the environment.\n4.4 Advanced Configuration: Python Environment Persistence\nA critical challenge in implementing the RLM Kernel is ensuring that the Python environment\npersists across different tool calls. By default, each python command in a shell might run in a\n--- PAGE 11 ---\nfresh process, losing variables defined in previous steps.\nTo solve this, we can leverage the PYTHONSTARTUP environment variable.27 By setting export\nPYTHONSTARTUP=$HOME/.claude/rlm_startup.py in the SessionStart hook or the user's shell\nprofile, we ensure that every time Claude invokes python, it pre-loads the RLM library and\npotentially re-hydrates state from a serialization file. This mimics the persistent state of a\nJupyter notebook kernel but within the robust, scriptable environment of the CLI.27 This allows\nClaude to define a variable dataset = load_data() in one turn and access dataset in the next,\nprovided the Python session is kept alive or state is restored.\nAdditionally, managing dependencies with modern tools like uv ensures that the environment\nis reproducible and fast.28 The SessionStart hook can verify that uv is installed and that the\nvirtual environment is active, preventing \"module not found\" errors that would break the\n\"Cannot Fail\" guarantee.\n5. Performance & Feasibility Analysis\n5.1 Cost Analysis\nThe RLM architecture introduces a fundamental trade-off: it increases the number of calls but\ndrastically reduces the context size per call.\nConsider the task of analyzing a 1 million token codebase.\n● Base Model Approach (GPT-5/Opus): Loading 1M tokens into the context window once\ncosts approximately $15 (assuming a hypothetical pricing of $15 per 1M input tokens).\nThis is a single, expensive point of failure.\n● RLM Approach (Sonnet/Haiku mix): The Root Agent sees only metadata and file lists (a\nvery small context). It then makes, for example, 100 sub-calls. However, these sub-calls\ncan be routed to cheaper, faster models like Haiku for simple extraction tasks (\"Find\nfunction definitions in this file\"). Or they can use small slices of Sonnet (e.g., 10k tokens).\n● Feasibility: The original RLM paper notes that RLM costs are \"comparable or cheaper\" to\nmonolithic calls.1 In the context of Claude Code, using claude -p with Haiku for the\nrecursive leaves (chunk processing) and Opus or Sonnet 3.5 for the root (synthesis)\ncreates a highly cost-effective pipeline.29 The cost becomes proportional to the relevant\ninformation extracted, rather than the total irrelevant data scanned.\n--- PAGE 12 ---\n5.2 Latency and Throughput\nA naive RLM implementation in a standard REPL is serial: the model issues a query, waits for\nthe result, and then issues the next. This blocks on every call.\nOptimization: Claude Code's Bash tool allows for background processes using the\nampersand (&) operator. The \"Cannot Fail\" architecture can leverage this to launch multiple\nclaude -p sub-tasks in parallel background processes. This creates a \"Map-Reduce\" style\nworkflow where the Root Agent spawns 10 workers to analyze 10 chapters simultaneously,\n--- PAGE 13 ---\nwaits for all to finish (wait command), and then aggregates the results. This parallelization\nsignificantly lowers the wall-clock time compared to a single massive serial stream, making\nthe RLM approach not just robust but also timely.25\n5.3 Security and Sandboxing\nThe feasibility of RLM also hinges on security. Allowing an LLM to execute code and spawn\nprocesses carries inherent risks.\nSandboxing: Claude Code's native sandboxing (built on OS primitives like bubblewrap on Linux\nor seatbelt on macOS) provides a crucial layer of defense.30 It restricts the agent's ability to\nread files outside the project directory or make arbitrary network connections.\nNetwork Policy: For the RLM to work, specifically if it uses claude -p (which requires\nauthentication), the sandbox must explicitly allow outbound connections to\napi.anthropic.com.31 The SessionStart hook or settings.json must be configured to whitelist\nthis domain, otherwise, the recursive calls will fail.\ndangerously-skip-permissions: While this flag enables fully autonomous \"YOLO mode\"\noperation 32, it is generally discouraged for the \"Cannot Fail\" architecture unless running\ninside a dedicated, disposable container (like a Docker container). The \"Cannot Fail\"\nphilosophy prioritizes safety and correctness over raw speed; therefore, the recommended\napproach is a configured sandbox with explicit allow-lists for the RLM Kernel tools, rather than\ndisabling all permission checks.\n6. Risk Mitigation: Why It \"Cannot Fail\"\nThe \"Cannot Fail\" moniker is bold. To justify it, the system must handle the worst-case\nscenarios inherent to agentic systems.\n6.1 Infinite Recursion Prevention\nRisk: The Root Agent delegates to a Sub-agent, which delegates back to the Root, creating an\ninfinite loop (fork bomb).33\nSolution: The RLM Kernel tracks a recursion_depth variable in the CLAUDE_ENV_FILE. The\nSessionStart hook increments this on every nested call. If recursion_depth > MAX_DEPTH\n(e.g., 3), the hook forces the session to abort or switch to a \"Leaf Mode\" where no further\ntools can be called, forcing a return value.1 This is a hard stop implemented in the shell\nenvironment, independent of the model's behavior.\n6.2 Context Explosion (The \"Blowing Up\" Risk)\nRisk: A sub-agent reads a file it thinks is small, but it's actually a 2GB log file, crashing the\nlocal memory or costing thousands of dollars.\nSolution: The PreToolUse hook defined in Section 4.1 acts as a mandatory firewall. No file read\nis permitted without a size check. Furthermore, the read tool is wrapped to return only the\nfirst 200 lines by default, requiring explicit paging for more.18 This ensures that \"accidental\"\nlarge reads are impossible.\n--- PAGE 14 ---\n6.3 Tool Hallucination\nRisk: The model tries to call a Python function that doesn't exist in the Kernel.\nSolution: The Kernel is loaded with a strictly typed interface (e.g., using Pydantic or similar\nvalidation in the Python script). If the model calls an invalid function, the REPL returns a\nstructured error with the valid schema, guiding the model back to the correct path\nimmediately.12 This self-correcting mechanism prevents the agent from getting stuck in a\nhallucination loop.\n7. Strategic Recommendations and Best Practices\nTo successfully deploy this architecture, we recommend the following phased approach:\n1. Phase 1: The \"Harness\" Development: Do not start with the model. Build the rlm_lib.py\nPython library first. Ensure your chunking, searching, and aggregation functions are\nrobust and unit-tested outside of Claude. The RLM is only as good as its Kernel.\n2. Phase 2: Hook Integration: Install the SessionStart and PreToolUse hooks. Test them\naggressively. Try to \"break\" the system by asking Claude to read a massive file. Ensure\nthe guardrails hold.\n3. Phase 3: Sub-agent Tuning: Experiment with claude -p calls. Determine the optimal\nprompt for your sub-agents. A lightweight model (Haiku) often performs better on\nfocused \"extract and summarize\" tasks than a reasoning model (Opus), reducing both\ncost and latency.\n4. Phase 4: Observable Telemetry: Use Claude Code's logging hooks (PostToolUse) to\nrecord every sub-call. You need visibility into the recursion tree to debug logic errors.18\nConclusion\nThe \"Cannot Fail\" RLM architecture is not a theoretical abstraction but a deployable reality\nwithin the Claude Code ecosystem. By leveraging the platform's persistent REPL, aggressive\nhook system, and sub-agent capabilities, developers can create an embedded system that\ntranscends the limitations of standard context windows. This architecture shifts the burden of\nmemory from the model's weights to the system's architecture, resulting in a robust, scalable,\nand economically viable solution for deep research and massive-scale code analysis. The key\nto success lies not in the model's intelligence, but in the rigidity of the environmental\nconstraints—the \"Cannot Fail\" guardrails—that surround it.\nWorks cited\n1. RLM RESEARCH PAPER.pdf\n2. Claude Code overview - Claude Code Docs, accessed on January 5, 2026,\nhttps://code.claude.com/docs/en/overview\n3. Subagents - Claude Code Docs, accessed on January 5, 2026,\nhttps://code.claude.com/docs/en/sub-agents\n4. Claude Code: A Simple Loop That Produces High Agency | by AI4HUMAN -\nMedium, accessed on January 5, 2026,\n--- PAGE 15 ---\nhttps://medium.com/@aiforhuman/claude-code-a-simple-loop-that-produces-hi\ngh-agency-814c071b455d\n5. Cooking with Claude Code: The Complete Guide - Sid Bharath, accessed on\nJanuary 5, 2026,\nhttps://www.siddharthbharath.com/claude-code-the-complete-guide/\n6. [DOCS] Environment variables don't persist between bash commands -\ndocumentation inconsistency · Issue #2508 · anthropics/claude-code - GitHub,\naccessed on January 5, 2026,\nhttps://github.com/anthropics/claude-code/issues/2508\n7. Introducing advanced tool use on the Claude Developer Platform - Anthropic,\naccessed on January 5, 2026,\nhttps://www.anthropic.com/engineering/advanced-tool-use\n8. Code execution tool - Claude Docs, accessed on January 5, 2026,\nhttps://platform.claude.com/docs/en/agents-and-tools/tool-use/code-execution-t\nool\n9. ClaudeJupy: Persistent Python & Jupyter for Claude AI - MCP Market, accessed\non January 5, 2026, https://mcpmarket.com/server/claudejupy\n10. Building agents with the Claude Agent SDK - Anthropic, accessed on January 5,\n2026,\nhttps://www.anthropic.com/engineering/building-agents-with-the-claude-agent-\nsdk\n11. How to use Claude Code subagents to parallelize development - Hacker News,\naccessed on January 5, 2026, https://news.ycombinator.com/item?id=45181577\n12. v0_bash_agent.py - shareAI-lab/learn-claude-code - GitHub, accessed on\nJanuary 5, 2026,\nhttps://github.com/shareAI-lab/learn-claude-code/blob/main/v0_bash_agent.py\n13. aichat: Claude-Code/Codex-CLI tool for fast full-text session search, and\ncontinue work without compaction : r/ClaudeAI - Reddit, accessed on January 5,\n2026,\nhttps://www.reddit.com/r/ClaudeAI/comments/1pylhtq/aichat_claudecodecodexcli\n_tool_for_fast_fulltext/\n14. Keeping AI Agents Grounded: Context Engineering Strategies that Prevent\nContext Rot Using Milvus, accessed on January 5, 2026,\nhttps://milvus.io/blog/keeping-ai-agents-grounded-context-engineering-strategi\nes-that-prevent-context-rot-using-milvus.md\n15. Effective context engineering for AI agents - Anthropic, accessed on January 5,\n2026,\nhttps://www.anthropic.com/engineering/effective-context-engineering-for-ai-ag\nents\n16. CLI reference - Claude Code Docs, accessed on January 5, 2026,\nhttps://code.claude.com/docs/en/cli-reference\n17. Trick to avoid context rot/dumber Claude Code sessions: New Hooks : r/ClaudeAI\n- Reddit, accessed on January 5, 2026,\nhttps://www.reddit.com/r/ClaudeAI/comments/1mib6o9/trick_to_avoid_context_ro\ntdumber_claude_code/\n--- PAGE 16 ---\n18. Get started with Claude Code hooks, accessed on January 5, 2026,\nhttps://code.claude.com/docs/en/hooks-guide\n19. SessionStart hooks not working for new conversations · Issue #10373 ·\nanthropics/claude-code - GitHub, accessed on January 5, 2026,\nhttps://github.com/anthropics/claude-code/issues/10373\n20. Modifying system prompts - Claude Docs, accessed on January 5, 2026,\nhttps://platform.claude.com/docs/en/agent-sdk/modifying-system-prompts\n21. Claude Code: Behind-the-scenes of the master agent loop - PromptLayer Blog,\naccessed on January 5, 2026,\nhttps://blog.promptlayer.com/claude-code-behind-the-scenes-of-the-master-ag\nent-loop/\n22. Introducing Pommel - an open source tool to help Claude Code find code without\nburning your context window : r/ClaudeAI - Reddit, accessed on January 5, 2026,\nhttps://www.reddit.com/r/ClaudeAI/comments/1q0gkn8/introducing_pommel_an_\nopen_source_tool_to_help/\n23. Feature Request: Automatic Context Restoration After Autocompaction · Issue\n#6066 · anthropics/claude-code - GitHub, accessed on January 5, 2026,\nhttps://github.com/anthropics/claude-code/issues/6066\n24. Hooks reference - Claude Code Docs, accessed on January 5, 2026,\nhttps://code.claude.com/docs/en/hooks\n25. The Ultimate Claude Code Guide: Every Hidden Trick, Hack, and Power Feature\nYou Need to Know - DEV Community, accessed on January 5, 2026,\nhttps://dev.to/holasoymalva/the-ultimate-claude-code-guide-every-hidden-trick-\nhack-and-power-feature-you-need-to-know-2l45\n26. Claude Code: Best practices for agentic coding - Anthropic, accessed on\nJanuary 5, 2026,\nhttps://www.anthropic.com/engineering/claude-code-best-practices\n27. A few Python REPL config tricks - DEV Community, accessed on January 5, 2026,\nhttps://dev.to/kenbellows/a-few-python-repl-config-tricks-3o6i\n28. Workaround for Claude Code running python instead of uv - Onur Solmaz blog,\naccessed on January 5, 2026,\nhttps://solmaz.io/log/2025/07/13/claude-code-python-override/\n29. A Guide to Claude Code 2.0 and getting better at using coding agents | sankalp's\nblog, accessed on January 5, 2026,\nhttps://sankalp.bearblog.dev/my-experience-with-claude-code-20-and-how-to-\nget-better-at-using-coding-agents/\n30. Making Claude Code more secure and autonomous with sandboxing - Anthropic,\naccessed on January 5, 2026,\nhttps://www.anthropic.com/engineering/claude-code-sandboxing\n31. Claude Code on the web, accessed on January 5, 2026,\nhttps://code.claude.com/docs/en/claude-code-on-the-web\n32. My setup for running Claude Code in YOLO mode without wrecking my\nenvironment - Reddit, accessed on January 5, 2026,\nhttps://www.reddit.com/r/ClaudeCode/comments/1pct552/my_setup_for_running\n_claude_code_in_yolo_mode/\n--- PAGE 17 ---\n33. Made Claude spawn its own sub-agents (recursive hierarchy with Claude Code\nCLI) : r/ClaudeAI - Reddit, accessed on January 5, 2026,\nhttps://www.reddit.com/r/ClaudeAI/comments/1pmp1lm/made_claude_spawn_its\n_own_subagents_recursive/"
}